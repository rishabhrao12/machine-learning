{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Machine Learning problems often have a large number of features involved. FOr larger problems this could be in thousands or even millions of features. Having such a arge number of features not only makes training on the data very slow but it could also make it harder for the model to find a good solution. We will prove this later. This problem is referred to as the *curse of dimensionality*.\n",
    "\n",
    "In real world problems however it is possible to reduce the number of features. For example if we are working with the MNIST dataset (dataset of images of handwritten numbers), the data is in 0 and 1s. The black part of the image which is the the actual number and the white part of the image which is the background. Obviously the background is irrelevant and so we can remove all the features whose value is 1. Also there is a strong correlation between neighboring pixels in data. So we can reduce the number of features by finding the mean of the 2 neighboring pixels and mege them into a single pixel. You will also not lose too much information.\n",
    "\n",
    "It is important to remember that reducing dimensionality does lead to some amount of information loss. We can intuitevely guess this, also we have seen it happen when we try to compress an image into the JPEG format and it loses some quality. This will speed up training but it could also lead to your model performing slightly worse. It also makes your pipelines more complex and hence harder to maintain. If training is too slow, you should firsst try to train the system using original data before you consider using dimensionality reduction. In some scrnarios reducing dimensionality of the training fata may remove noise and unnecessary details and could result in better performance this however will not happen generally, in most cases it will only speed up training.\n",
    "\n",
    "Apart from speeeding up training, dimensionality reduction can be extremely useful for data visualisation. Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of high dimensionality training data which could be useful for us to find some important insights by visually detecting patterns such as clusters. Data visualisation is also very important in order to communicate your conclusions to people who are not data scientist, such as decision makers who will use your results.\n",
    "\n",
    "This notebook will be divided into the following sections: -\n",
    "1. Curse of Dimensionality\n",
    "2. Main Approaches for Dimensionality Reduction\n",
    "3. PCA\n",
    "4. Kernel PCA\n",
    "5. LLE\n",
    "6. Other Dimensionality Reduction Techniques\n",
    "\n",
    "## 1. Curse of Dimensionality\n",
    "\n",
    "We are very used to living in three dimensions, which is why we cannot intuitively understand a higher dimensional space. For example we find it very difficult to picture even a basic 4D hypercube, let alone a 200 dimensional ellipsoid bent in a 1000 dimensional space.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Dimension_levels.svg/480px-Dimension_levels.svg.png\">\n",
    "</center>\n",
    "\n",
    "Things behave very differently when we are working in a high dimensional space. For example, if you pick a random point in a unit square, there is a about a 0.4% chance that it will be located less than 0.001 from a border (in other words it is very unlikely that the random point will be extreme along any dimension), however in a 10000 dimensional hypercube, this probability is greater than 99.999999%. Most points in a high dimensional space are very close to the border of the hypercube. (For a real world example: anyone you know is probably an extremist in at leat one dimension, for example how much sugar they put in their coffee, if you consider enough dimensions).\n",
    "\n",
    "An even bigger difference, is that if you pick two points randomly in a unit square, the distance between these two points will on average be roughly 0.52. If you pick two random points in a unit 3D cube, the average distance will be roughly 0.66. If you picked two points randomly in a 1,000,000-dimensional hypercube, the average distance will be about 408.25 roughly $\\sqrt{1,000,000/6}$. This is counterintiuitve, as how can two points be so far apart when they both lie in the same unit hypercube. The reason is that there is plently of space in high dimensions. As a result high-dimensional datasets are at risk of being very sparse, most training instances are likely to be far away from each other. This alsp means that a new instance will be far away from any training instance, making predictions much less reliable in lower dimensions since they will be based on large extrapolations. In short the more dimensions the training set has, the greater the risk of overfitting it.\n",
    "\n",
    "In theory one solution to this problem is to increase the size of the training set to reach a sufficient density of training instances. However in practice the number of training instances required to reach a given density grows exponentially with the number of dimensions. FOr example with just 100 features (significantally lesse than in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were all spread out uniformly across all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c21c860ba4f99eaf1f4c1d1bf1f659aaa3719ca3820e22e11ff4cdb72ac18dca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

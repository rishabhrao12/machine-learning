{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Machine Learning problems often have a large number of features involved. FOr larger problems this could be in thousands or even millions of features. Having such a arge number of features not only makes training on the data very slow but it could also make it harder for the model to find a good solution. We will prove this later. This problem is referred to as the *curse of dimensionality*.\n",
    "\n",
    "In real world problems however it is possible to reduce the number of features. For example if we are working with the MNIST dataset (dataset of images of handwritten numbers), the data is in 0 and 1s. The black part of the image which is the the actual number and the white part of the image which is the background. Obviously the background is irrelevant and so we can remove all the features whose value is 1. Also there is a strong correlation between neighboring pixels in data. So we can reduce the number of features by finding the mean of the 2 neighboring pixels and mege them into a single pixel. You will also not lose too much information.\n",
    "\n",
    "It is important to remember that reducing dimensionality does lead to some amount of information loss. We can intuitevely guess this, also we have seen it happen when we try to compress an image into the JPEG format and it loses some quality. This will speed up training but it could also lead to your model performing slightly worse. It also makes your pipelines more complex and hence harder to maintain. If training is too slow, you should firsst try to train the system using original data before you consider using dimensionality reduction. In some scrnarios reducing dimensionality of the training fata may remove noise and unnecessary details and could result in better performance this however will not happen generally, in most cases it will only speed up training.\n",
    "\n",
    "Apart from speeeding up training, dimensionality reduction can be extremely useful for data visualisation. Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of high dimensionality training data which could be useful for us to find some important insights by visually detecting patterns such as clusters. Data visualisation is also very important in order to communicate your conclusions to people who are not data scientist, such as decision makers who will use your results.\n",
    "\n",
    "This notebook will be divided into the following sections: -\n",
    "1. Curse of Dimensionality\n",
    "2. Main Approaches for Dimensionality Reduction\n",
    "3. PCA\n",
    "4. Kernel PCA\n",
    "5. LLE\n",
    "6. Other Dimensionality Reduction Techniques\n",
    "\n",
    "## 1. Curse of Dimensionality\n",
    "\n",
    "We are very used to living in three dimensions, which is why we cannot intuitively understand a higher dimensional space. For example we find it very difficult to picture even a basic 4D hypercube, let alone a 200 dimensional ellipsoid bent in a 1000 dimensional space.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Dimension_levels.svg/480px-Dimension_levels.svg.png\">\n",
    "</center>\n",
    "\n",
    "Things behave very differently when we are working in a high dimensional space. For example, if you pick a random point in a unit square, there is a about a 0.4% chance that it will be located less than 0.001 from a border (in other words it is very unlikely that the random point will be extreme along any dimension), however in a 10000 dimensional hypercube, this probability is greater than 99.999999%. Most points in a high dimensional space are very close to the border of the hypercube. (For a real world example: anyone you know is probably an extremist in at leat one dimension, for example how much sugar they put in their coffee, if you consider enough dimensions).\n",
    "\n",
    "An even bigger difference, is that if you pick two points randomly in a unit square, the distance between these two points will on average be roughly 0.52. If you pick two random points in a unit 3D cube, the average distance will be roughly 0.66. If you picked two points randomly in a 1,000,000-dimensional hypercube, the average distance will be about 408.25 roughly $\\sqrt{1,000,000/6}$. This is counterintiuitve, as how can two points be so far apart when they both lie in the same unit hypercube. The reason is that there is plently of space in high dimensions. As a result high-dimensional datasets are at risk of being very sparse, most training instances are likely to be far away from each other. This alsp means that a new instance will be far away from any training instance, making predictions much less reliable in lower dimensions since they will be based on large extrapolations. In short the more dimensions the training set has, the greater the risk of overfitting it.\n",
    "\n",
    "In theory one solution to this problem is to increase the size of the training set to reach a sufficient density of training instances. However in practice the number of training instances required to reach a given density grows exponentially with the number of dimensions. FOr example with just 100 features (significantally lesse than in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were all spread out uniformly across all dimensions.\n",
    "\n",
    "## 2. Main Approaches for Dimensionality Reduction\n",
    "\n",
    "Before discussing specific dimensionality reduction algorithms, we will look into the two main approaches to reducing dimensionality: Projection and Manifold Learning.\n",
    "\n",
    "### 2.1 Projection\n",
    "\n",
    "In most real world scenarios data is not uniformally spread across all dimensions. This is because many of the features are constant and many of them are highly correlated. As a result most instances lie within or close to a much lower dimensional subspace of the high demensional space. To get an idea we will look at the following example.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/63b8d7a91ff1ca2fdc35947a6a390c5a81085bd2//static/imgs/subspace_projection.png\" height=\"300\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Here we can notice that all the training instances lie close to a plane. This is a lower dimensional (2D) subspace of the high dimensional (3D) space. Now if we project all the training instances onto this subspace, we will get a 2D dataset (represented by the solid lines) which looks like the following. Notice that the axes correspond to new features $z_1$ and $z_2$ (the coordinates of the projections on the plane).\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/63b8d7a91ff1ca2fdc35947a6a390c5a81085bd2//static/imgs/2d_projection.png\" height=\"300\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "However projection is not always the best approach to dimensionality reduction. This is because sometimes the subspace may twist and turn which is seen in the very popular swiss roll toy dataset.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*GIUnQnlgfSe3vZMoRkg1ww.png\" height=\"400\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Simply projecting onto a plane, would squash different layers of the Swiss Roll together, as shown on the left side. However if we unroll the Swiss Roll along the axis of the plane, we get the 2D dataset on the right. This unrolling preserves the local structure of the Swiss Roll, but it distorts the global structure. This is why it is generally preferable to choose the axis to unroll along, rather than just projecting onto a plane.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*hXg4QP9Q0EsmJVNHi1DYsQ.png\" height=\"200\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "### 2.2 Manifold Learning\n",
    "\n",
    "The Swiss Roll dataset, that we have saw earlier is an example of a 2D manifold. A 2D manifold is a 2D shape that can be bent or twisted in a higher dimensional space. Generally a d-dimensional manifold is part of an n dimensional space where $d < n$ that locally resembles a d-dimensional hyperplane. In the case of a swiss roll, $d = 2, n = 3$, it locally resembles a 2D place but it is rolled in the third dimension.\n",
    "\n",
    "A hyperplane is a subspace whose dimensionality is 1 less than the ambient space around it. For example the hyperplane for a 3D space is a 2D plane while the hyperplane for a 2D space is a 1D space like a line.\n",
    "\n",
    "Many dimensionality reduction algorithms work by modelling the manifold on which the training instances lie. This is called Manifold Learning. It relies on the maniforld assumptio which is also called the maniforld hypothesis: states that most real-world high dimensional datasets lie close to a much lower dimensional manifold. This assumption is very often emperically observed.\n",
    "\n",
    "The manifold assumption is very often accompanied by another illicit assumption: that the task at hand (classification or regression for example) will be simpler if expressed in the lower dimensional space of the manifold. For example in the image given below where we try to classify the points, after we have unrolled the swiss roll the deicision boundary is a lot simpler. This is seen in the top row.\n",
    "\n",
    "This however does not always hold true, for example in the bottom row, the decision boundary for the higher dimensional space is at $x_1 = 5$. This decisiom boundary is very simple in the 3D space but it is more complex in the 2D unrolled manifold where it is made up of a collection of four independent line segments.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://pbs.twimg.com/media/EbNbDv5UcAg2c40.png\" height=\"400\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "## 3. Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it. The following figure shows the result of applying PCA to the 3D Swiss Roll dataset. The projection (represented by the dashed lines) preserves the distances between the instances.\n",
    "\n",
    "This section will be divided into the following subsections: -\n",
    "\n",
    "1. Preserving the Variance\n",
    "2. Principal Components\n",
    "3. Projecting Down to d Dimensions\n",
    "4. Using Scikit-Learn\n",
    "5. Explained Variance Ratio\n",
    "6. Choosing the Right Number of Dimensions\n",
    "7. PCA for Compression\n",
    "8. Randomized PCA\n",
    "9. Incremental PCA\n",
    "\n",
    "### 3.1 Preserving the Variance\n",
    "\n",
    "Before you project the training set onto a lower dimensional hyperplane, you first need to choose the right hyperplane. In the example simple 2D dataset given below, there are three different axes (1D hyperplanes). On the right is the result of the projection of the dataset onto each of the axes. As seen the projection onto the solid line preserves the maximum variance, while the projection onto the dotted line preserves the minimum variance. The dashed line preserves an intermediate amount of variance.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1400/0*qKESl4pUx0p6vqrv\" height=\"300\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "It seems reasonable to select the axis that preserved the maximum amount of variance, as it will likly lose the least amount of information as compared to the other projections. Another way to justify this choice is that it is the axis that minimises the mean squared distance between the original dataset and its projection onto that axis. This is the idea behind PCA.\n",
    "\n",
    "### 3.2 Principal Components\n",
    "\n",
    "PCA identifies the axis that acounts for the largest amount of variance in the training set. It is the solid line. It also finds a second axis orthogonal to the first one that accounts for the largest amount of remaining variance. For our above example this is the solid and dotted line. Solid line is chosen first we then need to find axis orthogonal to it so that is the dotted line only as the dashed line is not orthogonal. if it were a higher dimensional dataset, PCA would find a third axis, orthogonal to the previous two axes and a fourth, fifth and so on - as many axes ars the number of dimensions in the dataset.\n",
    "\n",
    "The $i^{th}$ axis is called the $i^{th}$ principal component (PC). The first PC is the axis on which vector $c_1$ lies and the second PC is the axis on which the vector $c_2$ lies. THe first two PCs are the orthogonal axes on which the two arrows lie, on the plane, and the third PC is the axis orthogonal to that plane.\n",
    "\n",
    "For each principal componnet, PCA find a zero-centered unit vector pointing in the direction of the PC. Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned bu PCA is not stable: if you perturb the training set slightly and run PCA again, the unit vectors may point in the opposite direction as the original vectors. However, they will generally lie on the same axes. In some cases, a pair of unit vectors may even rotate or swap (if the variances along the axes are close), but the plane they define will generally remain the same.\n",
    "\n",
    "So the question of how to find the prinicpal components of a training set arises. There is a standard matrix factorization technique called Singular Value Decomposition (SVD) that can decompose the training set matrix $X$ into the matrix multiplication of three matrices $U \\Sigma V^T$ where $V$ contains the unit vectors that efine all the principal components that we are looking for, as shown in the following equation.\n",
    "\n",
    "Equation: Principal Components Matrix\n",
    "\n",
    "$V = \\begin{bmatrix} c_1 & c_2 & \\dots & c_n \\end{bmatrix}$\n",
    "\n",
    "\n",
    "The following python code uses NumPys svd() function to obtain all the principal components of the training set, then extracts the two unit vectors that define the first two PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (100,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.linspace(start=-1., stop=1., num=100)\n",
    "y = X + np.random.normal(size=100)/7.\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y, c='blue')\n",
    "plt.xlabel('$X_1$')\n",
    "plt.ylabel('$X_2$')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X[..., None], y[..., None]), axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.67836909, 0.73472129]), array([ 0.73472129, -0.67836909]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n",
    "c1, c2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two 2D points represent the unit vector points (origin at (0,0)) corresponding to the 2 principal components (axes that preserve variance). PCA assumes that the dataset is centered around the origin, but scikit-learn implementation takes care of centering the data for you.\n",
    "\n",
    "PCA assumes the dataset is centered around the origin. As we will see later on if you implement PCA yourself, you will need to ensure that the data is centered first.\n",
    "\n",
    "### 3.3 Projecting Down to d Dimensions\n",
    "\n",
    "Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to $d$ dimensions by projecting it onto the hyperplane defined by the first $d$ principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible.\n",
    "\n",
    "For example in the figure given earlier, the 3D dataset is projected down to the 2D plane defined by the first two principal components. As a result the 2D projection looks very much like the original 3D dataset.\n",
    "\n",
    "To project the training set onto the hyperplane and obtain a reduced dataset $X_{d-proj}$ of dimensionality d, compute the matrix multiplication of the training set matrix $X$ by the matrix $W_d$, defined as the matrix containing the first d columns of $V$ as shown in the equation given below.\n",
    "\n",
    "Equation: Projecting the training set down to d dimensions\n",
    "\n",
    "$X_{d-proj} = XW_d$\n",
    "\n",
    "The following python code projects the training set onto the plane defined by the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to reduce the dimensionality of any dataset down to any number of dimensions, while preserving as much variance as possible.\n",
    "\n",
    "### 3.4 Using Scikit-Learn\n",
    "\n",
    "Scikit-Learn's PCA class implements PCA using SVD decomposition. The following code applies PCA to reduce the dimensionality of the dataset down to 2 dimensions. It automatically takes care of centering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the PCA transformer to the dataset, its componenets_ attribute holds the transpose of $W_d$ (e.g. the unit vector that defines the first principal componenet is qual to pca.components_.T[:, 0]).\n",
    "\n",
    "### 3.5 Explained Variance Ratio\n",
    "\n",
    "Another useful piece of information is the explained variance ratio of each principal component, available via the explained_variance_ratio_ variable. It indicates the proportion of the dataset's variance that lies along the axis of each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98660824, 0.01339176])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above scenario, the first principal component alone explains 98.7% of the variance, which is much more than the second principal component (0.12%): the first PC is much more informative than the second one.\n",
    "\n",
    "### 3.6 Choosing the Right Number of Dimensions\n",
    "\n",
    "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g. 95%). Unless of course, you are reducing dimensionality for data visualization - in that case you will generally want to reduce the dimensionality down to 2 or 3.\n",
    "\n",
    "The following code computes PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set's variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could then set n_components to d and run PCA again. There is however a much better way to do this. Instead of specifying the number of principal components you want to preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve. The following code computes the minimum number of dimensions required to preserve 95% of the training set's variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another option is to plot the explained variance as a function of the number of dimensions (simply plot the cumsum). There will usually be an elbow in the curve, where the explained variance stops growing fast. You can then select the number of dimensions that will add up to a good explained variance ratio. In this figure given below you can see that reducing the dimensionality down to 100 dimensions would not lose much explained variance.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/864/1*obeP0ttQR00oi3gRu2DgSA.png\" height=\"400\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "### 3.7 PCA for Compression\n",
    "\n",
    "After dimensionality reduction using PCA, the training set takes up much less space. As an example we can try applying PCA to the MNIST dataset while preserving 95% of the variance, each instance has 150 features,compared to the previous 784 features. SO while most of the vairnace is preserved the dataset is now less than 20% of its original size. This is a reasonable compression ratio and you can see how the sizr reduction can speed up training a classification algorithm such as an SVM classifier tremendously.\n",
    "\n",
    "It is also possible to decompress the reduced dataset back to 784 dimensions. However, the reconstructed data will not be the same as the original data, since the projection lost a bit of information (within the 5% variance that was dropped). The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the reconstruction error.\n",
    "\n",
    "The following code compresses the MNIST dataset down to 154 deimsnions, then decompresses it back to 784 dimensions. It uses the mean squared error as a performance measure of the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46900, 784), (23100, 784), (46900,), (23100,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = datasets.fetch_openml(name='mnist_784', return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure given below shows a few digits from the MNIST dataset, then the same digits after being compressed and decompressed. You can see that there is slight image quality loss but the digits are still mostly intact.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*QGCLtW_UVgF8iGyqFoU0KA.png\" height=\"300\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Equation: PCA inverse transformation, back to the original number of dimensions\n",
    "\n",
    "$X_{recovered} = X_{d-proj}W_d^T$\n",
    "\n",
    "### 3.8 Randomized PCA\n",
    "\n",
    "If you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses a stochastic algorithm called Randomized PCA that quickly finds an approximation of the first d principal components. Its computational complexity is $O(m \\times d^2) + O(d^3)$, instead of $O(m \\times n^2) + O(n^3)$ for the full SVD approach, so it is dramatically faster than full SVD when d is much smaller than n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver='randomized')\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, svd_solver is set to \"auto\", which means Scikit-Learn automatically uses the randomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m or n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use the full SVD approach, you can set the svd_solver hyperparameter to \"full\".\n",
    "\n",
    "### 3.9 Incremental PCA\n",
    "\n",
    "One problem with the preceding implementation of PCA is that it requires the whole training set to fit in memory in order for the SVD algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have been developed: you can split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time. \n",
    "\n",
    "This is useful for large training sets, and also to apply PCA online (i.e., on the fly, as new instances arrive).\n",
    "\n",
    "The following code splits the MNIST dataset into 100 mini-batches (using NumPy's array_split() function) and feeds them to Scikit-Learn's IncrementalPCA class to reduce the dimensionality of the MNIST dataset down to 154 dimensions. You must call the partial_fit() method with each mini-batch, instead of the usual fit() method with the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the NumPy memmap class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory: the class loads only the data it needs in memory, when it needs it. Since the IncrementalPCA class uses only a small part of the array at any given time, the memory usage remains under control. This makes it possible to call the fit() method, as seen in the following code. \n",
    "\n",
    "```\n",
    "X_mm = np.memmap(filename=filename, dtype='float32', mode='readonly', shape=(m,n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)\n",
    "```\n",
    "\n",
    "## 4 Kernel PCA\n",
    "\n",
    "The kernel trick is a mathematical technique that implicitly maps instances into a very high-dimensional space (called the feature space), enabling nonlinear classification and regression with Support Vector Machines. It is often useful with PCA as well, especially with complex nonlinear datasets. The same kernel trick we use earlier for SVMs can also be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. This is caled Kernel PCA (kPCA). It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold. The follwing code uses sklearns KernelPCA class to perform kPCA with an RBF kernel. The hyperparameter gamma controls how much the dataset is stretched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = datasets.make_swiss_roll(n_samples=1000, noise=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46900, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel='rbf', gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X_train)\n",
    "X_reduced.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure given below shows the Swiss Roll, reduced to two dimensions using a linear kernel (equivalent to using the PCA class), an RBF kernel and a sigmoid kernel. The linear kernel preserves the Swiss roll's shape, but the RBF kernel and the sigmoid kernel distort it. The RBF kernel preserves the Swiss roll's global shape, but it twists and stretches the roll in some places. The sigmoid kernel preserves the Swiss roll's global shape, but it twists and stretches the roll in some places. The sigmoid kernel preserves the Swiss roll's global shape, but it twists and stretches the roll in some places.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://img2018.cnblogs.com/blog/1012590/201904/1012590-20190406130102775-1737791858.png\" height=\"300\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "### 4.1 Selecting a Kernel and Tuning Hyperparameters\n",
    "\n",
    "Since kPCA is an unsupervised learning algorithm, tje is no obvious performance measure that we an use to help select the best kernel and hyerparameter values. Since dimensionalityb reduction si often used as a preparation step for a supervised learning task, such as classification, we can use grid search to select the kernel and hyperparameter values that leads to the best performance on that task. We can do this by creating a pipeline that first reduces dimensionality to 2 dimensions using kPCA, and then applies LogisticRegression for classification. It will then use GridSearchCV to find the kernel and gmma value for kPCA in order to get the best classification accuracy and the end of the pipeline. The following is a code implementation of the above idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;kpca&#x27;, KernelPCA(n_components=2)),\n",
       "                                       (&#x27;log_reg&#x27;, LogisticRegression())]),\n",
       "             param_grid=[{&#x27;kpca__gamma&#x27;: array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n",
       "       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n",
       "                          &#x27;kpca__kernel&#x27;: [&#x27;rbf&#x27;, &#x27;sigmoid&#x27;]}])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[(&#x27;kpca&#x27;, KernelPCA(n_components=2)),\n",
       "                                       (&#x27;log_reg&#x27;, LogisticRegression())]),\n",
       "             param_grid=[{&#x27;kpca__gamma&#x27;: array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n",
       "       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n",
       "                          &#x27;kpca__kernel&#x27;: [&#x27;rbf&#x27;, &#x27;sigmoid&#x27;]}])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;kpca&#x27;, KernelPCA(n_components=2)),\n",
       "                (&#x27;log_reg&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KernelPCA</label><div class=\"sk-toggleable__content\"><pre>KernelPCA(n_components=2)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('kpca', KernelPCA(n_components=2)),\n",
       "                                       ('log_reg', LogisticRegression())]),\n",
       "             param_grid=[{'kpca__gamma': array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n",
       "       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n",
       "                          'kpca__kernel': ['rbf', 'sigmoid']}])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('kpca', KernelPCA(n_components=2)),\n",
    "    ('log_reg', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    'kpca__gamma': np.linspace(0.03, 0.05, 10),\n",
    "    'kpca__kernel': ['rbf', 'sigmoid']\n",
    "}]\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid.fit(X_train[:10000], y_train[:10000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best kernel and hyperparameters are then available using the best_params_ variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kpca__gamma': 0.03, 'kpca__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach, which is entirely unsupervised, is to select the kernel and hyperparameter values that yield tje lowerst reconstruction error. Note that the reconstruction is not as easy as it is with linear PCA. This is because of the following reason. First check the diagram of Kernel PCA and the reconstruction pre-image error.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://nbviewer.org/github/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/master/static/imgs/reconstruction_vis.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "The original swiss roll is the top left image, and the resulting 2D dataset after kPCA is applied using an RBF kernel is top right. Thanks to te kernel trick, this transformation is mathematically equivalent to using the feature map $\\varphi$ to map the training sset to an infinite-dimensional feature space, then projecting the transformed training set down to 2D using linear PCA.\n",
    "\n",
    "If we could invert the linear PCA step for a given instance in the reduced space, the reconstructed point would lie in the feature space, not in the original space (e.g. like the one represented by an X in the diagram). Since the feature space is infinite-dimensional, we cannot compute the reconstructed point, and therefore we cannot compute the true reconstruction error. Fortunately it is possible to find a point in the original space that would map close to the reconstructed point, THis ic alled the reconstruction **pre-image**. once you have this pre-image you can measure its sequare distance to the original instance. You can then select the kernel and hyperparameters that minimize this reconstruction pre-image error.\n",
    "\n",
    "A question arises as to how we can perform this reconstruction, one solution is tro train a supervised regression model with the projected instances as the training set and the original instances as the targets. Sklearn wil do this automatically if you set fit_inverse_transform = True, as shown in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.0433, fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, fit_inverse_transform=False and KernelPCA has no inverse_transform() method. You can set fit_inverse_transform=True, but it will take much longer to train, and it will consume much more memory.\n",
    "\n",
    "Now we can compute the reconstruction pre-image error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.184110185279536"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 LLE\n",
    "\n",
    "Locally Linear Embedding (LLE) is another powerful nonlinear dimensionality reduction (NLDR) technique. It is a Manifold Learning technique that does not rely on projections like the previous algorithms. It works by first measureing how each training instance linearly relates to its closes neighbors and then looking for a low dimensional represnetation of the training set where these local relationships are best preserved. This approach makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise.\n",
    "\n",
    "The following code uses Scikit-Learn's LocallyLinearEmbedding class to unroll the Swiss roll, while trying to preserve the local relationships between instances. The n_neighbors hyperparameter tells how many neighbors each instance should be aware of. The figure given below shows the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting 2D dataset is shown in the figure given below. The Swiss roll is completely unrolled and the distenaces between instances are locally well preserved, However datasets are not preserved on a larger scale, the left part of the roll is stretched while the right part is squeezed. Regardless LLE did a pretty good job at modelling the manifold.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://nbviewer.org/github/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/master/static/imgs/swiss_unroll.png\" height=\"400\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "LLE works by: for each training instance $x^(i)$, the algorithm identifies its $k$ closest neighbors (in the code given above it was set as k = 10), then it tries to reconstruct $x^(i)$ as a linear function of these neighbors. To elaborate it finds the weights $w_{i, j}$ such that the mean squared distance between $x^(i)$ and $\\sum_{j=1}^{m} w_{i, j} x^(j)$ is as small as possible assuming $w_{i,j} = 0$ if $x^(j)$ is not one of the k closest neighbors of $x^(i)$. Thus the first step of LLE is the ocnstrained optimation problem given below where $W$ is the weight matrix contataining all the weights $w_{i,j}$. The second constraint simply normalises the weights for each training instance $x^(i)$.\n",
    "\n",
    "Equation: LLE step one: Linearly modelling local relationships\n",
    "\n",
    "$ \\hat{W} = argmin_{W} \\sum_{i=1}^{m} \\left\\| x^(i) - \\sum_{j=1}^{m} w_{i, j} x^(j) \\right\\|^2 $\n",
    "\n",
    "subject to $\\sum_{j=1}^{m} w_{i, j} = 1 $ for $i = 1,2,...,m$ and $w_{i, j} = 0$ if $x^{{(j)}$ is not one of the k closest neighbors of $x^{(i)}$\n",
    "\n",
    "After this step the weight matrix $\\hat{W}$ (containing the weights $\\hat{w}_{i,j}$) encodes the local linear relationship between the training instances. The second step is to map the training instances into $d$-dimensional space (where $d \\lt n$) while preserving these local relationships as much as possible. If $z^{(i)}$ is the image of $x^{(i)}$ in this $d$-dimensional space, then we want the squared distance between $z^{(i)}$ and $\\sum \\limits_{j=1}^{m} \\hat{w}_{i, j} z^{(j)}$ to be as small as possible. This idea leads to the unconstrained optimization problem given below. It looks very similar to the first step, but instead of keeping the instances fixed and finind optimal weights we are doing the reverse: keeping the weights fixed and finding the optimal positions of the instances images in the low-dimensional space. Note that $Z$ is the matrix containing all $z^{(i)}$.\n",
    "\n",
    "Equation: LLE step two: Reducing dimensionality while preserving relationships\n",
    "\n",
    "$ \\hat{Z} = argmin_{Z} \\sum_{i=1}^{m} \\left\\| z^{(i)} - \\sum_{j=1}^{m} \\hat{w}_{i, j} z^{(j)} \\right\\|^2 $\n",
    "\n",
    "Sklearns LLE implementation has the following computational complexity. $O(m \\log(m)n \\log(k))$ for finding the $k$ nearest eighbors, $O(mnk^3)$ for optimizing the weights and $O(dm^2)$ for constructing the low-dimensional representation Unfortunately the $m^2$ in the last term makes this algorithm scale poorly to very large datasets. However, if you set the n_neighbors hyperparameter to a very small value (e.g. 2), it will run much faster at the cost of a slightly worse reconstruction in the reduced dimensionality space.\n",
    "\n",
    "## 6 Other Dimensionality Reduction Techniques\n",
    "\n",
    "There are many other dimensionality reduction techniques. Several of which are available in Scikit-Learn. Some of the popular ones are as follows:\n",
    "\n",
    "1. Random Projections\n",
    "2. Multidimensional Scaling (MDS)\n",
    "3. Isomap\n",
    "4. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "5. Linear Discriminant Analysis (LDA)\n",
    "\n",
    "### 6.1 Random Projections\n",
    "\n",
    "Random Projections are a simple and fast dimensionality reduction technique. It projects the data to a lower-dimensional space using a random linear projection. This may sound horrible in practice but it turns out that uch a random projection is actually very likely to preserve the ditnces well, as was demonstrated mathematically in a famous lemma. The qualtiy of the dimensionality reduction depends on the number of instancea and the target dimensionality, but surprisingly not on the intitial dimensionality. It can be used using the sklearn.random_projection package.\n",
    "\n",
    "### 6.2 Multidimensional Scaling (MDS)\n",
    "\n",
    "Reduces dimensionality while trying to preserve the distances between the instances. It is a Manifold Learning technique that does not rely on projections like the previous algorithms. It works by first measureing how each training instance linearly relates to its closes neighbors and then looking for a low dimensional represnetation of the training set where these local relationships are best preserved. This approach makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise. Can be used using the sklearn.manifold package.\n",
    "\n",
    "### 6.3 Isomap\n",
    "\n",
    "Creates a graph by connecting each instance to its nearest neoghbors, then reduces dimensionality while trying to preserve the geodesic distances between the instances. The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between these nodes. Can be used using the sklearn.manifold package.\n",
    "\n",
    "### 6.4 t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. it is mostly used for visualisation, in particular to visualise clusters of instances in high-dimensional space (to visualise the MNIST images in 2D). Can be used using the sklearn.manifold package.\n",
    "\n",
    "### 6.5 Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Is a classification algorithm, but during training it learns the most discriminative axes between the classes and these axes an then be used to define the hyperplane onto which to project the data, The benefit of this approach is that the projection will keep the classes as far away as possible, so LDA is a good technique to reduce the deimnsionality before running another classification algorithm such as an SVM classifier. Can be used using the sklearn.discriminant_analysis package.\n",
    "\n",
    "## 7 Exercises\n",
    "\n",
    "1. Load the MNIST dataset and split it into a training set and a test set (take the first 60,000 instances for training and the remaining 10,000 for testing). Then train a Random Forest classifier on the training set and time how long it takes, then evaluate the resulting model on the test set. Next use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster? Next evaluate the classifier on the test set: how does it compare to the previous classifier? Hint: you can use the %timeit magic command to measure the execution time.\n",
    "2. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image's target class. Alternatively, you can replace each dot in the scatterplot with the corresponding instance's class (e.g., 0, 1, 2, etc.), or even plot scaled down versions of the digit images themselved (if you plot al digits the visualisation will be too cluttered so you should either draw a random sample or plot an nstance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well-separated clusters of digits. Now try using PCA, LLE, or MDS and compare the resulting visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c21c860ba4f99eaf1f4c1d1bf1f659aaa3719ca3820e22e11ff4cdb72ac18dca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Machine Learning problems often have a large number of features involved. FOr larger problems this could be in thousands or even millions of features. Having such a arge number of features not only makes training on the data very slow but it could also make it harder for the model to find a good solution. We will prove this later. This problem is referred to as the *curse of dimensionality*.\n",
    "\n",
    "In real world problems however it is possible to reduce the number of features. For example if we are working with the MNIST dataset (dataset of images of handwritten numbers), the data is in 0 and 1s. The black part of the image which is the the actual number and the white part of the image which is the background. Obviously the background is irrelevant and so we can remove all the features whose value is 1. Also there is a strong correlation between neighboring pixels in data. So we can reduce the number of features by finding the mean of the 2 neighboring pixels and mege them into a single pixel. You will also not lose too much information.\n",
    "\n",
    "It is important to remember that reducing dimensionality does lead to some amount of information loss. We can intuitevely guess this, also we have seen it happen when we try to compress an image into the JPEG format and it loses some quality. This will speed up training but it could also lead to your model performing slightly worse. It also makes your pipelines more complex and hence harder to maintain. If training is too slow, you should firsst try to train the system using original data before you consider using dimensionality reduction. In some scrnarios reducing dimensionality of the training fata mayremove noise and unnecessary details and could result in better performance this however will not happen generally, in most cases it will only speed up training.\n",
    "\n",
    "Apart from speeeding up training, dimensionality reduction can be extremely useful for data visualisation. Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of high dimensionality training data which could be useful for us to find some important insights by visually detecting patterns such as clusters. Data visualisation is also very important in order to communicate your conclusions to people who are not data scientist, such as decision makers who will use your results.\n",
    "\n",
    "This notebook will be divided into the following sections: -\n",
    "1. Curse of Dimensionality\n",
    "2. Main Approaches for Dimensionality Reduction\n",
    "3. PCA\n",
    "4. Kernel PCA\n",
    "5. LLE\n",
    "6. Other Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c21c860ba4f99eaf1f4c1d1bf1f659aaa3719ca3820e22e11ff4cdb72ac18dca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "A Support Vector Machine commonly reffered to as an SVM is a powerful and very versatile machine learning model. SVM are one of the most popular Supervised Learning algorithms, they are capable of linear and non linear classification, regression and even outlier detection. Support Vector Machines are typically well suited for classification of complex small or medium sized datasets.\n",
    "\n",
    "The goal of SVM is to create the best line or decision boundary that can be used to segregate n-dimensional space into classes so that we can easily put the new datapoint in the correct category in the future. This best decision boundary is called the hyperplane. SVM chooses extreme points/vectors (that are instances) that help create the hyperplane, these are called support vectors, hence the name of the algorithm. This process will be discussed in more detail later.\n",
    "\n",
    "This chapter will be divided into the following sections: -\n",
    "\n",
    "1. Linear SVM Classification\n",
    "2. Nonlinear SVM Classification\n",
    "3. SVM Regression\n",
    "4. Working of SVM\n",
    "\n",
    "## 1. Linear SVM Classification\n",
    "\n",
    "<br>\n",
    "<img src=\"https://www.researchgate.net/publication/304611323/figure/fig8/AS:668377215406089@1536364954428/Classification-of-data-by-support-vector-machine-SVM.png\" width=\"400\">\n",
    "<br>\n",
    "\n",
    "The fundamental idea behind SVM can best be explained by using the above image as an example. The two classes can easily be seperated by a straight line (i.e they are linearly seperable). \n",
    "\n",
    "The decision boundaries are marked by the dash lines. As stated earlier the support vectors are the ones that are used to make the hyperplanes. The instances are classified according to which side of the hyperplane they lie on. You can think of an SVM classifier as fitting the widest possible street (represented by parallel dashed lines) between the classes. This is called large margin classification. It should be noted that addeing more training instances off the street will not affect the decision boundary at all. Ot is fully determined/supported by the instances located on the edge of the street. These instances are called the support vectors.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781787125933/files/graphics/B07030_03_09.jpg\" height=\"350\">\n",
    "<br>\n",
    "\n",
    "It must also be rememebered that SVMs are sensitive to feature scales, after feature scaling the decision boundary looks a lot better, i.e. the margin is a lot bigger.\n",
    "\n",
    "https://miro.medium.com/max/1332/1*mKH7ePxH9xJ2Avsess9nzA.png\n",
    "\n",
    "<br>\n",
    "<img src=\"https://miro.medium.com/max/1332/1*mKH7ePxH9xJ2Avsess9nzA.png\" height=\"250\">\n",
    "<br>\n",
    "\n",
    "### 1.1 Soft Margin Classification\n",
    "\n",
    "We cannot always make the model in such a way that all the instances are off the street (street referring to the area between the hyperplanes). If we try to impose this then it is called hard margin classification. The two main issues with hard margin classification are that: 1. It only works with linearly seperable data, 2. It is highly susceptible to outliers. For example if one of the instances of negative classes is in the group of the positive class instances when we plot, then it becomes impossible to seperate them in such a way to have hard margin i.e. no instances on the street. Another problem is that if we do make the decision boundary taking the outlier into account then the model will overfit and will have a hard time generalising.\n",
    "\n",
    "To avoid these issues we use a more flexible model. The objective is to find a model that keeps the street as wide as possible and limit the margin violations (i.e. instances that end up in the middle of the street or even on the wrong side). This is called soft margin classification.\n",
    "\n",
    "When creating a SVM model using sklearn, we can specify a number of hyperparameters. C is one of those hyperparameters. If we set it to a low value, then we end up with the modelthat will have a wide street but a few instances will be on the street. IF we set it to a higher value the street will be a lot narrower and we will however have fewer instances on the street. The prior model would be have more margin violations but will probably generalise better. Our goal is to find a good balance between these two. If the SVM model is overfitting, then you can regularise it by reducing the value of C.\n",
    "\n",
    "Next lets implement Linear SVM Model and train it on the iris dataset. We will load the dataset, scale the feature and then train a Linear SVM Model (using LinearSVC class with C=1 and the hinge loss function) to detect Iris Virginica flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2, 3)]  # to get petal length and petal width\n",
    "y = (iris['target'] == 2).astype(float)\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear_svc', LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Logistic Regression classes, SVM classifiers do not put probabilities of each class. Instead of using the LinearSVC class, we could also the SVC class with a linear kernel. To do this we would write SVC(kernel=\"linear\", C=1). Or we could use the SGDClassifier class, with SGDClassifier(loss=\"hinge\", alpha=1/(m*C)). This applies regular Stochastic Gradient Descent to train a linear SVM classifier. It does not converge as fast as the linear SVC class, but it can be used to handle online classification tasks or huge datasets that do not fit in memory (out-of-core training).\n",
    "\n",
    "The LinearSVC class regularises the bias term, so you shoukld center your training set first by subtracting its mean. THis is automatic if you train the data using the standard scaler. Also make sure the loss hyperparameter is set to \"hinge\" as it is not the default value. For better performance, you should set the dual hyperparameter to False unless there are more features than training instances.\n",
    "\n",
    "\n",
    "## 2. Nonlinear SVM Classification\n",
    "\n",
    "Linear SVM Classifiers are efficient and work very well, however a vast majority of daatasets are not even close to being linearly seperable. One approach to handling non linear datasets, is to add more features, such as polynomial features, in some cases this can resilt in a linearly seperable dataset. For example a dataset with just one feature x1 may not be linearly seperable but if another feature is added, the resulting 2D dataset is perfectly linearly seperable.\n",
    "\n",
    "<br>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*NwhqamsvzBkUlYwSAubv5g.png\" height=\"300\">\n",
    "<br>\n",
    "\n",
    "To implement this using sklearn, we will create a Pipeline containing a PolynomialFeatures transformer, followed by a Standard Scaler and a Linear SVC. We can test this on the moons dataset: a toy dataset for binary classification in which data points are shaped as two interleaving half circles. You can generate this dataset using the make_moons() function.\n",
    "\n",
    "We will now implement this in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;poly_features&#x27;, PolynomialFeatures(degree=3)),\n",
       "                (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_clf&#x27;, LinearSVC(C=10, loss=&#x27;hinge&#x27;, max_iter=10000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;poly_features&#x27;, PolynomialFeatures(degree=3)),\n",
       "                (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_clf&#x27;, LinearSVC(C=10, loss=&#x27;hinge&#x27;, max_iter=10000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures(degree=3)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=10, loss=&#x27;hinge&#x27;, max_iter=10000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge', max_iter=10000))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "poly_svm_clf = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', LinearSVC(C=10, loss=\"hinge\", max_iter=10000))  # failed to converge, use a value less than 1k\n",
    "])\n",
    "\n",
    "poly_svm_clf.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src=\"https://img2018.cnblogs.com/blog/1012590/201903/1012590-20190331183702438-196976647.png\" height=\"400\">\n",
    "<br>\n",
    "\n",
    "### 2.1 Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c21c860ba4f99eaf1f4c1d1bf1f659aaa3719ca3820e22e11ff4cdb72ac18dca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

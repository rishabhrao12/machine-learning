{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Regression is a supervised learning task similar to classification. The major distinction between the two is that regression is (in most cases) used to predict a target variable which is continuous. An example problem would be of trying to predict the real estate price of a house based on features like the number of bathrooms, the distance from the highway e.t.c. This notebook focuses both on how to use premade models that will automatically find the values of the parameters, and on how to create implementations of algorithms that you can create as the user to find out how the models work under the hood.\n",
    "\n",
    "In this notebook the following topics will be covered: -\n",
    "\n",
    "1. Linear Regression\n",
    "2. Gradient Descent\n",
    "3. Polynomial Regression\n",
    "4. Learning Curves\n",
    "5. Regularised Linear Models\n",
    "6. Logistic Regression\n",
    "\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "A simple linear regression problem is very easy to understand. As stated earlier, regression is used to predict target values which are continuous. For example predicting the value of a house.\n",
    "\n",
    "A linear model makes a prediction simply by computing a weighted sum of the input features, in addition to a bias term (called an intercept). A linear model hence has the same equation of that of the slope of the line equation (y = mx + c). This is because the logic of a linear model is to create a line that best fits the data, i.e. straight line that goes through most of the data.\n",
    "<br>\n",
    "\n",
    "$\\hat{y} = \\theta_{0}  + \\theta_{1}x_{1} + \\theta_{2}x_{2} + .. + \\theta_{n}x_{n}$\n",
    "<br>\n",
    "\n",
    "In this equation:\n",
    "* $\\hat{y}$ is the predicted value\n",
    "* n is the number of features\n",
    "* $x_{i}$ is the ith feature variable\n",
    "* $\\theta_{j}$ is the jth model parameter (including the bias term $\\theta_{0}$ and the feature weights $\\theta_{1}, \\theta_{2}$...)\n",
    "\n",
    "This can be written in a much simpler fashion using vectorized form:\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\hat{y} = h_{\\theta}(x) = \\theta.x$\n",
    "\n",
    "<br>\n",
    "\n",
    "In this equation:\n",
    "\n",
    "* $\\theta$ is the models parameter vector containing the bias term $\\theta_{0}$ and the feature vectors $\\theta_{1}$ to $\\theta_{n}$\n",
    "* $x$ is the instance features vector containing $x_{0}$ to $x_{n}$ with $x_{0}$ always equal to 1.\n",
    "* $\\theta.x$ is the dot product of vectors \\theta and x which is of course equal to $\\theta_{0}x_{0}  + \\theta_{1}x_{1} + \\theta_{2}x_{2} + .. + \\theta_{n}x_{n}$\n",
    "* $h_{\\theta}$ is the hypothesis function, using the model parameters $\\theta$\n",
    "\n",
    "\n",
    "This is the lineaar regression model. TO train it we set its parameters so that the model best fits the training set. We need a measure of how well the model performs so that we can tell whether it is improving or not. For lassification we used metrics like Accuracy, F1, Precision, Recall ..., for Regression we will be using errors like MSE (mean squared error), RMSE (root mean squared error) e.t.c. TO train a Linear Regression Model we need to find te value of $\\theta$ that minimises the RMSE. It is actually simpler to minimise the MSE than the RMSE and it leads to the same result. MSE is calculated in the following manner.\n",
    "\n",
    "<br>\n",
    "\n",
    "$MSE (X, h_{\\theta}$) =  $\\frac{1}{m}$ * $\\sum \\limits _{i=1} ^ {m}$ ($\\theta^T {x^i} - {y^i})^2$\n",
    "\n",
    "<br>\n",
    "\n",
    "To simply understand it, MSE is where we first find the error for each entry that is every y_true - y_pred, we then square this error and find the mean of these errors.\n",
    "\n",
    "### 1.1 Normal Equation\n",
    "\n",
    "To find the value of $\\theta$ that minimises the cost function there is a closed form solution - in other words, a mathematical equation that gives the result directly. This is called a normal equation.\n",
    "\n",
    "<br>\n",
    "\n",
    "$h_{\\theta} = {({X^T}X)^{-1}} X^T y$\n",
    "\n",
    "<br>\n",
    "\n",
    "Eq:\n",
    "* $\\hat{\\theta}$ is the value of $\\theta$ that minimises the cost function\n",
    "* $y$ is the vector of target values containing $y_{1}$ to $y_{m}$\n",
    "\n",
    "To test this equation we will generate some linear looking data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7d790295b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute $\\hat{\\theta}$ using the Normal equation. We will use inv() function from NumPy linear algerba model to compute the inverse of matrix, and the dot method for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # adds x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that we used to generate the data is $y = 4 + 3x_{1} +$ Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.05427354],\n",
       "       [2.92167107]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above the model did not find the exact values instead it came quite close. In an ideal scenario we would have gotten values as 4 and 3. The reason we couldnt find it exactly was because of the noise.\n",
    "\n",
    "Now to make predictions using $\\hat{\\theta}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.05427354],\n",
       "       [9.89761568]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0],[2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # adds x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_new, y_predict, 'r-')\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Linear Regression using Scikit-Learn is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.05427354]), array([[2.92167107]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "model.intercept_, model.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model also approximated almost the same values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.05427354],\n",
       "       [9.89761568]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_new)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression callss is based on scipy.linalg.lstsq() which stands for least squares (that is the method used to find param values)\n",
    "\n",
    "### 1.2 Computation Complexity\n",
    "\n",
    "The normal equation computes the inverse of $X^TX$ which is an (n + 1) x (n + 1) matrix (where n is the number of featyres). The computational complexity of inverting such a matrix is $O(n^{2.4})$ to $O(n^{3})$, depending on the implementation. In other words if you double the number of features you multiply the computational time by $2^2.4 = 5.3$ to $2^3 = 8$\n",
    "\n",
    "The SVD approach used by sklearn LinearRegression class is about $On^{2}$. If you double the number of features you multiply the computational time by roughly 4.\n",
    "\n",
    "Both normal and SVD get slow as features grow large. But on a poitive side both are linear with regard to the number of instances in the training set (O(m)) so they handle large training sets efficiently provided they can fit in memory.\n",
    "\n",
    "Once the model is trained, predictions are very fast, predictions on twice as many instances or features will takes twice as much time.\n",
    "\n",
    "We will now learn how to train a Linear Regression model which is better suited for cases where there are a large number of features or too many training instances to fit in memory.\n",
    "\n",
    "\n",
    "## 2. Gradient Descent\n",
    "\n",
    "Gradient Descent is a generic optimization algorithm capable of finding optimum solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function.\n",
    "\n",
    "As an example if you are at the top of a hill and it is surrounded by fog and you can only feel the slope of the ground under your feet, you will keep walking downhill in the direction of the steepest slope. This is exactly what Gradient Descent does: it measures the local gradient of the error function with regard to the parameter vector $\\theta$, and it goes in the direction of descending gradient. Once the gradient is zero you have reached a minimum.\n",
    "\n",
    "Concretely you start by filling $\\theta$ with random values (called random intialisation). Then you improve it gradually taking one step at a time each attempting to decrease the cost function until the algorithm converges to a minimum.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/0*fU8XFt-NCMZGAWND.\" alt=\"Gradient Descent\" width=300></center>\n",
    "<br>\n",
    "\n",
    "An important parameter in Gradient Descent is the size of the steps, determing by the **learning rate** parameter. If learning rate is too small, the algorithm will have to go through several iterations to converge which will take a long time. On the other hand if the learning rate is too high, you might jump cross the valley and end up even higher than before. This might make the algorithm diverge with larger and larger values failing to find a good solution\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://i.stack.imgur.com/au77i.png\" alt=\"Gradient Descent\" height=300></center>\n",
    "<br>\n",
    "\n",
    "Not all cost functions look like regular bowls as seen above instead there may be holes, ridges, plateaus and all sorts of irregularities which makes convergence to minimum difficult. Sometimes you could end up findind the local minimum which is not the same as the global minimum and you could also end up stopping thw iterative process before finding the global minimum.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://www.i2tutorials.com/wp-content/media/2019/09/Neural-network-32-i2tutorials.png\" alt=\"Gradient Descent\" height=300></center>\n",
    "<br>\n",
    "\n",
    "Fortunately the MSE cost function for Linear Regression model happens tp be a covex function which means that if you pick any two points on the curve the line segment joining the two points never crosses the curve. This implies that there are no local minimum only one global minimum.It is also a continuous function with a slope that never changes abruptly. These two facts have a major consequence, that is Gradient Descent is guarenteed to approach arbitarily close to the global minimum (if enough iterations, and learning rate is not too high)\n",
    "\n",
    "The cost function has the shape of a bowl, but it can be an elongated bowl if features have different scales. \n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://www.oreilly.com/api/v2/epubs/9781491962282/files/assets/mlst_0406.png\" alt=\"Gradient Descent\" height=300></center>\n",
    "<br>\n",
    "\n",
    "The left one reaches it quickly as it goes straight to the minimum and the right one without the scaling does not do the same rather it goes in an almost orthogonal direction compared to the global minimum. It will reach the global minimum but it will take a long time.\n",
    "\n",
    "Therefore it is important to use a scaler (sklearn StandardScaler) to ensure features all have a similar scale before fitting the data.\n",
    "\n",
    "This also illustrates the fact that training a model means searching for a combination of model parameters that minimises a cost function (over the training set). It is a search in the models parameter space. The more parameters a model has the more dimensions the space has, and the harder the search is.\n",
    "\n",
    "### 2.1 Batch Gradient Descent\n",
    "\n",
    "To implement Gradient descent you need to compute the gradient of cost function with regard to each model paramter $\\theta_{j}$. Simply put you need to calculate how much the cost fucntion will change if you change $\\theta_{j}$ a little bit. This is called partial derivative. Taking the previous mountain slope example, it is like asking what is slope if you face east, and then the same for north, south ... and so on for all dimensions depending in the scenario. The following equation computes the cost function with regard to parameter $\\theta_{j}$ noted $\\frac{\\partial{MSE(\\theta)}}{\\partial{\\theta_{j}}}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\frac{\\partial{MSE(\\theta)}}{\\partial{\\theta_{j}}} = \\frac{2}{m}\\sum \\limits_{i=1}^{m} (\\theta^Tx^{(i)}-y^{(i)})x_{j}^{(i)}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Instead of computing these partial derivates individually you can compute them all in one go. The gradient vector noted $\\nabla_{\\theta}MSE(\\theta)$, contains all the partial derivates of the cost function (one for each model parameter).\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\nabla_{\\theta}MSE(\\theta) = [\\frac{\\partial{MSE(\\theta)}}{\\partial{\\theta_{0}}} + \\frac{\\partial{MSE(\\theta)}}{\\partial{\\theta_{1}}} + .. \\frac{\\partial{MSE(\\theta)}}{\\partial{\\theta_{n}}}] = \\frac{2}{m}X^T(\n",
    "X\\theta - y)$\n",
    "\n",
    "<br>\n",
    "\n",
    "Note: Remember that this formula includes the calculations over the full training set X at each Gradient Descent Step. This is why the training algorithm is called Batch Gradient Descent, it uses the whole batch of training data at every step (Full Gradient Descent is a more apt name). This is as a result very slow on large training sets. However Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation or SVD decomposition.\n",
    "\n",
    "Once you have the gradient vector which points uphill just go in the opposite direction to go downhill that meas subtract $\\nabla_{\\theta}MSE(\\theta)$ from $\\theta$.\n",
    "\n",
    "This is where learning rate $\\eta$ is important: multiply the gradient vector by $\\eta$ to determine the size of the downhill step.\n",
    "\n",
    "$\\theta^{(next step)} = \\theta - \\eta\\nabla_{\\theta}MSE(\\theta)$\n",
    "\n",
    "To implement this in python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.05427354],\n",
       "       [2.92167107]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1)   # random initialisation\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is exactly what the normal equation found. Gradient Descent therefore worked perfectly.\n",
    "\n",
    "To find a good learinng rate you can use grid search. However it is a good decision to limit the number of iterations so that grid search can eliminate models that take too long to converge.\n",
    "\n",
    "To know how many iterations to use, it is good practice to set a large number of iterations and to interrupt the algorithm when the gradient vector becomes very tiny ie the norm becomes smaller than a tiny number $\\epsilon$ called the tolerance, this happens when Gradient Descent has almost reached the minimum.\n",
    "\n",
    "When the cost function is convex and its slope does not change abrptly (as is the case for MSE cost function), Batch Gradient Descent with a fixed learning rate will eventually converge to the optimal solution but you may have to wait a while $O(1/\\epsilon)$ iterations to reach the optimum within a range of $\\epsilon$, depending on the shape of the cost function. If you divide the tolerance by 10 to have a more precise solution the algorithm may have to run about 10 times longer.\n",
    "\n",
    "### 2.2 Stochastic Gradient Descent\n",
    "\n",
    "The main problem with Batch Gradient Descent is that it uses the whole training set to compute the gradients at every step, which makes it very slow when the set is large. Stochastic gradient Descent is on the opposite extreme, it takes a random instance every step of the way and computes the gradients for that step on just that single instance. This makes it uch faster as it needs to load only one instance in memory. It is also makes it possible to train on large training sets.\n",
    "\n",
    "Due its stochastic (random) nature it does not smoothly go towards the minimum but will instad go up and down, however it will reduce on an average, even once it reaches the minimum it will continue to bounce around. The final values hence are good but not optimal.\n",
    "\n",
    "When the cost function is ery irregular this helps as the algorithm will jump ourt of the local minima so Stochastic Gradient Descent has a better chance of finding global minima than Batch Descent.\n",
    "\n",
    "The randomness therefore is good to escape local minima but hard to reach a optimal solution. To counter this we can change the learning rate, by using a large learning rate in the beginning to escape from local minmium and a smaller one as time passes on to reach an optimum solution, The function that determines the learning rate is called the learning schedule.\n",
    "\n",
    "To implement stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.09017657],\n",
       "       [2.91327529]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1)  # random initialisation\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate by rounds of m iterations, each round is called an epoch. While the Batch gradient Descent code iterated 1000 times through the whole training ser. This code goes through it 50 times and reaches a pretty good solution.\n",
    "\n",
    "It is important to remember that since instances are piocked randomly some instances may be picked multiple timesper epoch while other may not be picked. Be sure that algorithm goe through every instance at each epoch or shuffle the training set. Then go through instance by instance and shuffle it again.\n",
    "\n",
    "To perform linear regression using stochastic gd use the SGDRegressor class of sklearn which defaults to optimising the squared error cost function. The following implemntation does not use any regularisation (penalty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.96528224]), array([2.85201897]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3,penalty=None, eta0=0.1)\n",
    "# eta0 - starting learning rate, tol - will run till loss drops below this value\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Mini-Batch Gradient Descent\n",
    "\n",
    "Mini batch gradient descent is like a combination of the previous two gradient descents. Unlike SGD mini batch gradient descent trains on small random sets of instances of the data at every step, these are called mini-batches. The main advantage of SGD is that they can get a performance boost from hardware optimisation of matrix operations especially while using GPUs.\n",
    "\n",
    "Algorithms progress in parameter space is less erratic than with SGD. However mini-batch finds it harder to escape the local minima and cotinues to walk around even after reaching solution space. Must remember that this can be overcome by using a good learning schedule.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*9calCrrqS9opiytuA--7AA.png\" alt=\"Gradient Descent\" height=300></center>\n",
    "<br>\n",
    "\n",
    "A comparison of algorithms would be as follows: -\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://pic4.zhimg.com/v2-b921c7e730e0578e3fd0322ef9d0d53b_b.jpg\" alt=\"Gradient Descent\" height=300></center>\n",
    "<br>\n",
    "\n",
    "\n",
    "## 3. Polynomial Regression\n",
    "\n",
    "There are certain scenarios where your datas feature variables and target variable do not have a linear realtionship, rather the data could be plotted as a curve. In this scenarios we cannot use a linear solution, however we could change a linear model in such a way to work with non-linear data. This can be done simply by adding powers of each feature as new features, then training a linear model on this extended set of features. This technique is called Polynomial Regression.\n",
    "\n",
    "For example:\n",
    "$\\hat{y} = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{1}^2 + ... + \\theta_{n}x_{1}^n$\n",
    "\n",
    "Generating some non-linear data using a quadratic equation and some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7d5a423910>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X ** 2 + X + 2 +np.random.randn(m, 1)\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed a straight line cant fit this data properly, so we will use Scikit-Learns PolynomialFeatures class to transform our training data, adding the square (second degree polynomial) of each feature in the training set as a new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.82966936]), array([-2.82966936,  8.00702871]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0], X_poly[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_poly now contains the original feature of X plus the square of this feature. Now you can fit a Linear Regression model to this extended training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.12370305]), array([[1.01270211, 0.46098737]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_poly = LinearRegression()\n",
    "lin_reg_poly.fit(X_poly, y)\n",
    "lin_reg_poly.intercept_, lin_reg_poly.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model estimates $\\hat{y} = 0.46x_{1}^2 + 1.01x_{1} + 2.12$ when the actual function was $y = 0.5x_{1}^2 + 1.0x_{1} + 2.0 + $ Gaussian noise\n",
    "\n",
    "**NOTE IMP:** When there are multiple features Polynomial Regression is capable of finding relationships between features which is something a plain Linear Regression mode cannot do. This is made possible by the fac that PolynomialFeatures also adds all combinations of features upto the given degree. For example if there were two features a, b, Polynomial Featurs with degree 3 would not only add the features $a^{2}, a^{3}, b^{2}, b^{3}$, but also all the combinations $ab, a^2b, ab^2$\n",
    "\n",
    "PolynomialFeatures (degree=d) transforms an array containing n features into an array containing $\\frac{(n+d)!}{d!n!}$ where $n!$ is the factorial of n = 1 x 2 x .. x n. As a result there will be a large number of combinations which could make this model slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c21c860ba4f99eaf1f4c1d1bf1f659aaa3719ca3820e22e11ff4cdb72ac18dca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "A Support Vector Machine commonly reffered to as an SVM is a powerful and very versatile machine learning model. SVM are one of the most popular Supervised Learning algorithms, they are capable of linear and non linear classification, regression and even outlier detection. Support Vector Machines are typically well suited for classification of complex small or medium sized datasets.\n",
    "\n",
    "The goal of SVM is to create the best line or decision boundary that can be used to segregate n-dimensional space into classes so that we can easily put the new datapoint in the correct category in the future. This best decision boundary is called the hyperplane. SVM chooses extreme points/vectors (that are instances) that help create the hyperplane, these are called support vectors, hence the name of the algorithm. This process will be discussed in more detail later.\n",
    "\n",
    "This chapter will be divided into the following sections: -\n",
    "\n",
    "1. Linear SVM Classification\n",
    "2. Nonlinear SVM Classification\n",
    "3. SVM Regression\n",
    "4. Working of SVM\n",
    "\n",
    "## 1. Linear SVM Classification\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "<img src=\"https://www.researchgate.net/publication/304611323/figure/fig8/AS:668377215406089@1536364954428/Classification-of-data-by-support-vector-machine-SVM.png\" height=\"400\">\n",
    "<br>\n",
    "</center>\n",
    "\n",
    "The fundamental idea behind SVM can best be explained by using the above image as an example. The two classes can easily be seperated by a straight line (i.e they are linearly seperable). \n",
    "\n",
    "The decision boundaries are marked by the dash lines. As stated earlier the support vectors are the ones that are used to make the hyperplanes. The instances are classified according to which side of the hyperplane they lie on. You can think of an SVM classifier as fitting the widest possible street (represented by parallel dashed lines) between the classes. This is called large margin classification. It should be noted that addeing more training instances off the street will not affect the decision boundary at all. Ot is fully determined/supported by the instances located on the edge of the street. These instances are called the support vectors.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781787125933/files/graphics/B07030_03_09.jpg\" height=\"350\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "It must also be rememebered that SVMs are sensitive to feature scales, after feature scaling the decision boundary looks a lot better, i.e. the margin is a lot bigger.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1332/1*mKH7ePxH9xJ2Avsess9nzA.png\" height=\"350\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "\n",
    "### 1.1 Soft Margin Classification\n",
    "\n",
    "We cannot always make the model in such a way that all the instances are off the street (street referring to the area between the hyperplanes). If we try to impose this then it is called hard margin classification. \n",
    "\n",
    "The two main issues with hard margin classification are that: -\n",
    "1. It only works with linearly seperable data \n",
    "2. It is highly susceptible to outliers. \n",
    "\n",
    "For example if one of the instances of negative classes is in the group of the positive class instances when we plot, then it becomes impossible to seperate them in such a way to have hard margin i.e. no instances on the street. Another problem is that if we do make the decision boundary taking the outlier into account then the model will overfit and will have a hard time generalising.\n",
    "\n",
    "To avoid these issues we use a more flexible model. The objective is to find a model that keeps the street as wide as possible and limit the margin violations (i.e. instances that end up in the middle of the street or even on the wrong side). This is called soft margin classification.\n",
    "\n",
    "When creating a SVM model using sklearn, we can specify a number of hyperparameters. C is one of those hyperparameters. If we set it to a low value, then we end up with the model that will have a wide street but a few instances will be on the street. IF we set it to a higher value the street will be a lot narrower and we will however have fewer instances on the street. The prior model would be have more margin violations but will probably generalise better. Our goal is to find a good balance between these two. If the SVM model is overfitting, then you can regularise it by reducing the value of C.\n",
    "\n",
    "Next lets implement Linear SVM Model and train it on the iris dataset. We will load the dataset, scale the feature and then train a Linear SVM Model (using LinearSVC class with C=1 and the hinge loss function) to detect Iris Virginica flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2, 3)]  # to get petal length and petal width\n",
    "y = (iris['target'] == 2).astype(float)\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear_svc', LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Logistic Regression classes, SVM classifiers do not put probabilities of each class. Instead of using the LinearSVC class, we could also the SVC class with a linear kernel. To do this we would write SVC(kernel=\"linear\", C=1). Or we could use the SGDClassifier class, with SGDClassifier(loss=\"hinge\", alpha=1/(m*C)). This applies regular Stochastic Gradient Descent to train a linear SVM classifier. It does not converge as fast as the linear SVC class, but it can be used to handle online classification tasks or huge datasets that do not fit in memory (out-of-core training).\n",
    "\n",
    "The LinearSVC class regularises the bias term, so you should center your training set first by subtracting its mean. This is automatic if you train the data using the standard scaler. Also make sure the loss hyperparameter is set to \"hinge\" as it is not the default value. For better performance, you should set the dual hyperparameter to False unless there are more features than training instances.\n",
    "\n",
    "\n",
    "## 2. Nonlinear SVM Classification\n",
    "\n",
    "Linear SVM Classifiers are efficient and work very well, however a vast majority of datasets are not even close to being linearly seperable. One approach to handling non linear datasets, is to add more features, such as polynomial features, in some cases this can result in a linearly seperable dataset. For example a dataset with just one feature x1 may not be linearly seperable but if another feature is added, the resulting 2D dataset is perfectly linearly seperable.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*NwhqamsvzBkUlYwSAubv5g.png\" height=\"300\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "To implement this using sklearn, we will create a Pipeline containing a PolynomialFeatures transformer, followed by a Standard Scaler and a Linear SVC. We can test this on the moons dataset: a toy dataset for binary classification in which data points are shaped as two interleaving half circles. You can generate this dataset using the make_moons() function.\n",
    "\n",
    "We will now implement this in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;poly_features&#x27;, PolynomialFeatures(degree=3)),\n",
       "                (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_clf&#x27;, LinearSVC(C=10, loss=&#x27;hinge&#x27;, max_iter=10000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;poly_features&#x27;, PolynomialFeatures(degree=3)),\n",
       "                (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_clf&#x27;, LinearSVC(C=10, loss=&#x27;hinge&#x27;, max_iter=10000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures(degree=3)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=10, loss=&#x27;hinge&#x27;, max_iter=10000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge', max_iter=10000))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "poly_svm_clf = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', LinearSVC(C=10, loss=\"hinge\", max_iter=10000))  # failed to converge, use a value less than 1k\n",
    "])\n",
    "\n",
    "poly_svm_clf.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center>\n",
    "<img src=\"https://img2018.cnblogs.com/blog/1012590/201903/1012590-20190331183702438-196976647.png\" height=\"400\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "### 2.1 Polynomial Kernel\n",
    "\n",
    "Adding polynomial features is very useful as because of this we can get our models to fit and perform well on non-linear data. Adding these features make models work great and it is also very simple to implement. However at a low polynomial degree, this method cannot deal with very complex datasets, and at large degrees it becomes very slow because of the combinatorial explosion, creating a large number of features, making the model very slow.\n",
    "\n",
    "There is however a trick to get around this which is called the kernel trick. The kernel trick makes it possible to get the same result as if you had added many polynomial features without actually adding them. So there is no combinatorial explosion and the model does not become slow. Trick can be impleemented using the SVC class. We implement it in the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_clf&#x27;, SVC(C=5, coef0=1, kernel=&#x27;poly&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_clf&#x27;, SVC(C=5, coef0=1, kernel=&#x27;poly&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=5, coef0=1, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svc = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel='poly', degree=3, coef0=1, C=5))\n",
    "])\n",
    "\n",
    "poly_kernel_svc.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains an SVM classifier using a third degree polynomial kernel. If your model is overfitting, then reduce the degree, if underfitting you can increase it. The hyperparameter coef0 controls how much the model is influenced by high-degree polynomials vs low-degree polynomials.\n",
    "\n",
    "A common approach to find the right hyperparameter value is to use grid search. It is often faster to do a very coarse grid search, then user a finer grid search around the best values found. Hvaing a good sense of what each hyperparameter does can also help search the right part of the hyperparameter space.\n",
    "\n",
    "\n",
    "### 2.2 Similarity Features\n",
    "\n",
    "Another technique to tackle nonlinear problems is to add features computed using a **similarity function**, which measures how much each instance resembles a particular landmark. For example taking the 1D dataset and adding two landmarks to it at $x_{1} = -2$ and $x_{2} = 1$. Next we will define a similarity function to be the Gaussian Radial Basis Function (RBF) with $\\gamma = 0.3$\n",
    "\n",
    "Gaussian RBF:\n",
    "\n",
    "$\\phi_{\\gamma}(x, \\ell) = \\exp(-{\\gamma} || x - \\ell || ^2)$\n",
    "\n",
    "This is a bell shaped function varying from 0 (very far away from landmark) to 1 (at the landmark). Now we can compute the new features. For example taking instance $x_{1} = -1$, it is located at a distance of 1 from the first landmark and 2 from the second landmark. Therefore its new features are $x_{2} = exp(-0.3 x 1^2) \\approx 0.74$ and $x_{3} = exp(-0.3 x 2^2) \\approx 0.30$. The right plot shows the transformed dataset without the original features. Now it is observed that it is linearly seperable.\n",
    "\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1306/1*_9go20Mzy5ECwLi-syNrYw.png\" height=\"400\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "The main question that arises is how do we choose the landmarks. The simplest approach is to make a landmark at the position of every instance in the dataset. Doing so will create many dimensions and thus the changes of our instances being linearly seperable increases. The problem that arises with this is that if your dataset has n feature and m instances before we perform the process after we do so, there will be m features and m instances, assuming you drop the original features. This will make it linearly seperable but can cause problems if the dataset is very large, as it will after the transformation have an equally large number of features.\n",
    "\n",
    "\n",
    "### 2.3 Gaussian RBF Kernel\n",
    "\n",
    "Just like the polynomial features method, the similarity features method can be used with any machine learning algorithm, it could be computationally heavy because of the large number of features that would be created if you used a large dataset. Once again we use the kernel trick, that makes it possible to obtain a similar result as if you added many similarity features. An implementation of the SVC Class with the Gaussian RBF kernel would be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_clf&#x27;, SVC(C=0.001, gamma=5))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_clf&#x27;, SVC(C=0.001, gamma=5))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=0.001, gamma=5)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is represented at the bootom. The other plots are other models that were trained on other gamma $\\gamma$ and C values. Increasing the value of gamma makes the bell shaped curve narrower as a result the decision boundary becomes more irregular and tends to very closelt go around the instances. This is because as you increase the value of gamma, each instances range of influence becomes smaller, when you reduce gamma the influence of individual instances becomes larger and the bell curve becomes smoother. If your model is overfitting, you should reduce the alue of gamma, i.e. increase the influence of individual instances, which will make the bell curve smoother and it wont overfit.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://img2018.cnblogs.com/blog/1012590/201903/1012590-20190331223623432-1521850923.png\" height=\"400\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Other kernels are also there but they are rarely used. Some kernels are specialised ofr specific data structures. String kernels are sometimes used when classifying text documents or DNA sequences (using the string subsequence kernel or kernels based on the Levenshtein distance).\n",
    "\n",
    "There are multiple kernels to choose from : linear, rbf, sigmoid ... So you might be confused on which one to choose. You must always start off by trying to use the linear kernel, (NOTE: LinearSVC is much faster than SVC(kernel=\"linear\")), this is especially true if your training set is very large or you have a large amount of features. if the training set is not too large, you can also try using Gaussian RBF Kernel, it works well in most cases. After this if you yet want to experiment you can use cross validation and grid search to find what suits your data. Some kernels work better for specific data structures as stated earlier.\n",
    "\n",
    "\n",
    "### 2.4 Computational Complexity\n",
    "\n",
    "The LinearSVC class is based on the liblinear library, it implements an optimised algorithm for Linear SVMs but it does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features. Its training time is roughly $O$ ($m$ x $n$)\n",
    "\n",
    "The algorithm takes longer if you require a very high precision. This is controlled by the tolerance hyperparameter $\\epsilon$ (called tol in sklearn). In most classification tasks, the default tolerance is fine.\n",
    "\n",
    "The SVC class is based on the libsvm library, which implements an algorithm that supports the kernel trick unlike the LinearSVC class. The training time complexity is between $O$ ($m^2$ x $n$ ) and $O$ ($m^3$ x $n$).\n",
    "\n",
    "Because of this time complexity, we know that it gets very slow as the number of training instances get large. This algorithm is perfect for handling complex small - medium sized datasets. It scales well with the number of fatures especially with sparse features (each instance has few non-zero features). In this case algorithm scakes roughly with the average number of nonzero features per instance.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://img2018.cnblogs.com/blog/1012590/201903/1012590-20190331234705484-1218971349.png\" height=\"250\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "\n",
    "## 3. SVM Regression\n",
    "\n",
    "The SVM algorithm is very versatile, not only can it be used for classification tasks, it can also be used for regression tasks. The goal however is opposite, while in classification we tried to make the street as large as possible and margin violations were the instances that were in the street, in regression we try to make the street as narrow as possible and fit in all the instances in the street, here margin violations are when the instances are not there in the street but are outside it.\n",
    "\n",
    "The width of the street is controlled by a hyperparameter called $\\epsilon$, the figure given below hows two linear svm regression models trained on some random linear data, one with  alarge margin and the other with a small margin. Adding more training instances within the margin does not afect the models performance that is the model is $\\epsilon$-insensitive.\n",
    "\n",
    "\n",
    "You can use sklearns LinearSVR class to perform linear SVM regression. Always remember to scale and center the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_reg&#x27;, LinearSVR(epsilon=1.5))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svm_reg&#x27;, LinearSVR(epsilon=1.5))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVR</label><div class=\"sk-toggleable__content\"><pre>LinearSVR(epsilon=1.5)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_reg', LinearSVR(epsilon=1.5))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_reg', LinearSVR(epsilon=1.5))\n",
    "])\n",
    "\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle nonlinear regression tasks, you can use a kernelised SVm model. The following diagram shows SVM regression on a random quadratic training set, we use a second degree polynomial kernel. There is a litte regularisation in the left plot (i.e. a much larger C value), and much more regularisation in the right plot (a small C value).\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*1MvBY0uWo69Z2vfFF6hCNA.png\" height=\"300\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Following we have an implementation that uses sklearn SVR class (supports kernel trick) to produce the model thats on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(C=100, degree=2, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(C=100, degree=2, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel='poly', degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVR class is the regression equivalent of the SVC class, and the LinearSVR class is the regression equivalent of the LinearSVC class. The LinearSVR class scales linearly with the size of the training set (like LinearSVC class), while the SVR class gets too slow when the training set grows large (like th SVC class).\n",
    "\n",
    "SVMs can be also be used for outlier detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working of SVM\n",
    "\n",
    "We will now further explore how SVMs make predictions and how their training algorithms work starting with linear SVM classifiers.\n",
    "\n",
    "First we will start by looking into notations. Earlier in regression we discussed the convention of putting all the model parameters in one vector $\\theta$, including the bias term $\\theta_{0}$ and the input feature weights $\\theta_{1}$ to $\\theta_{n}$, and adding a bias input $x_{0}= 1$ to all instances. Now weill use a convention that is more convenient and common when dealing with SVMs. The bias term will be called b, and the feature weights vector will be w. No bias feature will be added to the input feature will be added to the input feature vetors.\n",
    "\n",
    "### 4.1 Decision Funtion and Predictions\n",
    "\n",
    "The linear SV< classifier model predicts the class of a new instance x by computing the decision function: \n",
    "\n",
    "$w^Tx + b = w_{1}x_{1} + ... + w_{n}x_{n}$. \n",
    "\n",
    "If the result is positive, the predicted class $\\hat{y}$ is the positive class (1), else it is the negative class (0).\n",
    "\n",
    "$\\hat{y} = 0 if w^Tx + b < 0 or 1 if w^Tx + b >= 0$\n",
    "\n",
    "The decision fucntion is given below. THe decision boundary is the set of points where the decision fucntion is equal to 0, it is the intersection of two planes, which is a straight line.\n",
    "\n",
    "Dashed lines represent the points where the decision fcuntion is equal to 1 or -1, they are parallel and at equal distance to the decision boundary, and they fprm a margin around it. Training a SVM classifier means finding the values of w and b that make this margin as wide as possible while avoiding margin violations (hard magin) or limiting them (soft margin).\n",
    "\n",
    "### 4.2 Training Objective\n",
    "\n",
    "If we consider the slope of a decision function: it is equal to the norm of the weight vector $||w||$. If we divide this slope by 2, the points where the decision function is equal to $\\pm1$ are going to be twice as far away from the decision boundary. This means that dividing the slope by 2 will multiply the margin by 2. The smaller the weight vector $w$, the larger the margin.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://miro.medium.com/max/1240/1*0HncfVU3eh_eU_CsTL9XPg.png\" height=\"300\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "So the goal is to minimise $|| w || $ to get a large margin. If we want to avoid any margin violations (hard margin), then we need the decision fucntion to be greater than 1 for all positive training instances and lower than -1 for negative training instances. If we define $t^{(i)} = -1$ for negative instances (if $y^{i} = 0$) and $t^{(i)} = 1$ for positive instances (if $y^{i} = 1$), then we can express this constraint as $t^{(i)}(w^Tx^{(i)} + b \\ge 1)$ for all instances.\n",
    "\n",
    "Equation for Hard margin linear SVM classifier objective: \n",
    "\n",
    "$\\frac{minimize}{w, b} \\frac{1}{2}w^Tw$\n",
    "\n",
    "subject to:  $t^{(i)}(w^Tx^{(i)} + b) \\ge 1$   for i = 1,2,.. m\n",
    "\n",
    "To get the soft margin objective we need to introduce a slack variable $\\xi^{(i)} \\ge 0$ for each instance, $\\xi^{(i)}$ measures how much the $i^{th}$ instance is allowed to violate the margin. We now have two conflicting objectives, make the slack variables as small as possible to reduce the margin violations and make $\\frac{1}{2}w^Tw$ as small as possible to increase the margin. This is where the hyperparameter C comes in, it allows us to define the tradeoff between these two objectives. This gives us the constrained optmization problem.\n",
    "\n",
    "Equation for Soft margin linear SVM classifier objective: \n",
    "\n",
    "$\\frac{minimize}{w, b, \\xi} \\frac{1}{2}w^Tw + C \\sum \\limits_{i=1}^{m} \\xi^{(i)}$\n",
    "\n",
    "subject to:  $t^{(i)}(w^Tx^{(i)} + b) \\ge 1 - \\xi^{(i)}$ and $\\xi^{(i)} \\ge 0$  for i = 1,2,.. m\n",
    "\n",
    "\n",
    "### 4.3 Quadratic Programming\n",
    "\n",
    "The hard margin and spft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are known as Quadratic Programming problems.\n",
    "\n",
    "Minimise  $\\frac{1}{2}p^THp + f^Tp$ \n",
    "p subject to: $Ap \\le b$\n",
    "\n",
    "where,\n",
    "* $p$ is an $n_p$ dimensional vector where ($n_p$ = number of parameters)\n",
    "* $H$ is an $n_p x n_p$ dimensional vector\n",
    "* $f$ is an $n_p$ dimensional vector\n",
    "* $A$ is an $n_c x n_p$ dimensional vector where ($n_c$ = number of constraints)\n",
    "* $b$ is an $n_c$ dimensional vector\n",
    "\n",
    "One way to train a hard margin linear SVM classifier is to use an off the shelf QP solver and pass it the various parameters. The resulting vector p will contain the bias term $b = p_{0}$ and the feature weights $w_{i} = p_{i}$ for i = 1,2, .. n. Similarly you can use a QP solver to solve the soft margin problem. To use the kernel trick however you will need to look at a different optimisation problem.\n",
    "\n",
    "### 4.4 The Dual Problem\n",
    "\n",
    "Given a constrained optimisation problem, known as the primal problem, it is possible to express a different but closely related problem, called its dual problem. The solution to the gual problem typically givea a lower bound to the solution of the primal problem but under some conditions it can have the same solution as the primal problem. Luckily the SVM problem meets these conditions (i.e the objective function is convex and thre inequality constraints are continuously differentiable and convex functions), so you can choose to solve the primal problem or the dual problem, both will have the same solution.\n",
    "\n",
    "Dual Solution of the linear SVM objective\n",
    "\n",
    "$\\frac{minimise}{\\alpha} \\frac{1}{2} \\sum \\limits_{i=1}^{m} \\sum \\limits_{j=1}^{m} \\alpha^{i} \\alpha^{j} t^i t^j x^{(i)T} x^j - \n",
    "\\sum\\limits_{i=1}^{m} \\alpha^{i}$\n",
    "\n",
    "subject to $\\alpha^{i} \\ge 0$ for i = 1,2,..,m\n",
    "\n",
    "Once you find the vector $\\hat{\\alpha}$ that minimises this equation (use QP solver), use the following equation to compute $\\hat{w}$ and $\\hat{b}$ that minimises the primal problem.  \n",
    "\n",
    "$\\hat{w} = \\sum \\limits_{i=1}^{m} \\hat{\\alpha}^{i}t^i x^i $\n",
    "\n",
    "$\\hat{b} = \\frac{1}{n_s} \\sum \\limits_{i=1}^{m} (t^i - \\hat{w}^Tx^i $\n",
    "\n",
    "where $\\hat{\\alpha}^i > 0$\n",
    "\n",
    "The dual problem is faster to solve than the primal one when the number of training instances is smaller than the number of fetures. More importantly the dual problem makes the kernel trick possible, while the primal does not. Now we will look into the kernel trick."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Kernelised SVMs\n",
    "\n",
    "in the scenario where you want to apply a second degree polynomial transformation to a two dimensional training dataset, then train a linear SVM classifier on the transformed training set. The following equation shows the second degree polynomial mapping function $\\phi$ that you want to apply.\n",
    "\n",
    "The transformed vector is 3D instead of 2D. Now lets see what happens to a couple of 2D vectors, a and b if we apply this second degree polynomial mapping and then compute the dot product of the transformed vectors.\n",
    "\n",
    "The dot product of the transformed vectors is equal to the square of the dot product of the original vectors: $\\phi(a)^T\\phi(b) = (a^Tb)^2$\n",
    "\n",
    "The key insight is: if you apply the transformation \\phi to all training instances then the dual problem will contain the dot product $\\phi(x^i)^T\\phi(x^j)$ but if the $\\phi$ is the second degree polynomial transformation then you can replace the dot product of transformed vectors simply by $(x^{(i)T}x^{(j)})^2$. So you dont need to transform all the training instances, just replace the dot product by its square. The result will be strictly the same as if you had gone through the trouble of transforming the training set then fitting a linear SVM algorithm but this trick makes the whole process much more computationally efficient.\n",
    "\n",
    "The function $K(a, b) = (a^Tb)^2$ is a second degree polynomial kernel. In machine learning, a kernel is a fucntion capable of computing the dot product $\\phi(a)^T \\phi(b)$, based only on the original vectors a and b wthout having to compute or even know about the transformation $\\phi$. The following are some of the most widely used kernels: \n",
    "\n",
    "* Linear: $K(a, b) = a^Tb$\n",
    "* Polynomial: $K(a, b) = (\\gamma a^Tb + r)^d$\n",
    "* Gaussian RBF: $K(a, b) = exp(-\\gamma || a - b ||^2)$\n",
    "* Sigmoid: $K(a, b) = tanh(\\gamma a^Tb + r)$\n",
    "\n",
    "There is a further process to convert this to the primal solution.\n",
    "\n",
    "\n",
    "### 4.6 Online SVMs\n",
    "\n",
    "This is the final section on SVMS and it is on Online SVM Classifiers (online means it learns incrementally, as new instances arrive).\n",
    "\n",
    "For linear SVM classifiers, one method for implementing an online SVm classifier is to use Gradient Decent (e.g. using SGDClassifier) to minimise the cost function given beloew which is derived from the primal problem. Unfortunately Gradient Descent converges much slower than the methods based on QP.\n",
    "\n",
    "Linear SVM classifier cost function:\n",
    "\n",
    "$J(w,b) = \\frac{1}{2}w^Tw + C\\sum \\limits_{i=1}^{m}\\max(0, 1-t^{(i)}(w^Tx^{i} + b))$\n",
    "\n",
    "The first sum in the cost function will push the model to have a small weight vector w, leading to a larger margin. The second sum computes the total of all margin violations. An instances margin violation is equal to 0 if it is located off the street and on the correct side or else it is proportional to the ditance to the correct side of the street. Minimising this term ensures the model makes the margin violations as small and as few as possible.\n",
    "\n",
    "it is also possible to implement online kernelised SVMs, as described in papers \"incremental and Decremental Support Vector Machine Learning\" and \"Fast Kernel Classifiers with Online and Active Learning\". These kernelised SVMs are implemented in Matlab and C++. For large scale problems, you may want to use neural networks instead.\n",
    "\n",
    "\n",
    "**Hinge Loss:** the function max(0, 1- t) is called the hinge loss function. It is equal to 0 when t %/ge% . Its derivative (slope) is equal to -1 if t < 1 and 0 if t > 1. It is not differentiable at t = 1, but just like for Lasso Regression, you can still use Gradient Descent using any subderivative at t=1 (any value between -1 and 0).\n",
    "\n",
    "<br>\n",
    "<img src=\"https://datamonje.com/wp-content/uploads/2022/01/Hinge-loss.png\" height=\"350\">\n",
    "<br>\n",
    "\n",
    "\n",
    "## 5. Exercises\n",
    "\n",
    "1. Train a LinearSVC on a linearly seperable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model.\n",
    "2. Train an SVM classifier on the MNIST dataset. Since the SVM classifiers are binary, you will need to use the one-vs-rest to classify for all ten digits. You will also have to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you achieve? (sklearn dataset)\n",
    "3. Train an SVM regressor on the California housing dataset (sklearn dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c21c860ba4f99eaf1f4c1d1bf1f659aaa3719ca3820e22e11ff4cdb72ac18dca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
